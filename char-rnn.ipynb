{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load in some text to use\n",
    "shakes = open('data/shakespeare.txt', 'r').read()\n",
    "print(shakes[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'y', 'h', 'l', 'W', 'z', 'N', '&', ':', 'L', 'M', 'u', 'G', '3', 'V', 't', 'b', 'Y', 'x', 'F', 'C', 'K', 'p', ' ', '.', ';', 'D', 'j', 'B', 'f', 'm', 'J', 'X', 'a', 'g', '$', 'S', 'r', 'E', '!', 'i', 'P', '-', \"'\", 'c', 'T', 'Z', '\\n', 'd', 'v', 'R', 'H', '?', 'O', 'A', 'Q', 'o', 'U', 'k', 'w', 'e', ',', 's', 'q', 'I']\n"
     ]
    }
   ],
   "source": [
    "# need to get all of the possible characters that the source uses\n",
    "chars = list(set(shakes))\n",
    "data_size, vocab_size = len(shakes), len(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to convert from characters to index and from index back to characters\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some hyperparameters for our network\n",
    "hidden_size = 2048\n",
    "seq_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GRU in tensorflow\n",
    "\n",
    "In this final notebook we'll work in Tensorflow directly. I would recommend getting familiar with how neural networks work by using our previous examples and then once you feel comfortable with Keras and all of the high level concepts move into Tensorflow. \n",
    "\n",
    "Tensorflow does give us a few helper functions to facilitate the construction of neural networks, but mostly we will be building lots of things from scratch. The one thing that we definitely don't want to do is calculate the backward pass for our training steps, luckily this is something that Tensorflow will do for us. \n",
    "\n",
    "In this example we will create a GRU recurrent neural network to use in our character level RNN. The steps for creating this network from scratch will be:\n",
    "\n",
    "* Initialize all of our weight matrices. Setup their sizes and fill with random numbers\n",
    "* Define the calculations that our network must carry out\n",
    "\n",
    "A GRU cell is basically a change in the way that the hidden state is calculated for a recurrent neural network. So to begin we'll start with a vanilla recurrent neural network and show how we can create one using the two steps above. \n",
    "\n",
    "### Vanilla RNN\n",
    "\n",
    "The calculations for a recurrent neural network look like the following:\n",
    "\n",
    "![rnn](images/rnn.png)\n",
    "\n",
    "In order to create that we need to set up three matrices and two bias vectors. The specify the calculations in exactly the same way. \n",
    "\n",
    "```python\n",
    "Uh = tf.get_variable(\"Uh\", [input_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "bh  = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "by  = tf.get_variable(\"by\", [output_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "hs_t = tf.tanh(tf.matmul(xs_t, Uh) + tf.matmul(hs_t, Wh) + bh)\n",
    "ys_t = tf.nn.softmax(tf.matmul(hs_t, Vy) + by)\n",
    "```\n",
    "\n",
    "Simple enough. Input_size and output_size will change depending on the properties of our data. Hidden_size is a hyperparameter that we can set to anything that we wish.\n",
    "\n",
    "### Gated Recurrent Unit\n",
    "\n",
    "The changes to go from a vanilla RNN to a GRU are made to the way that we calculate the hidden state. Therefore, we will keep all of the matrices that we set up earlier and just add in what we need for the GRU calcualtions. The GRU calculations look as follows:\n",
    "\n",
    "![gru](images/GRU.png)\n",
    "\n",
    "Lets add in what we need.\n",
    "\n",
    "```python\n",
    "# weight from input to hidden for z gate\n",
    "Uz = tf.get_variable(\"Uz\", [vocab_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# weight from hidden to hidden for z gate\n",
    "Wz = tf.get_variable(\"Wz\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# bias for the z gate calculation\n",
    "bz = tf.get_variable(\"bz\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# weight from input to hidden for r gate\n",
    "Ur = tf.get_variable(\"Ur\", [vocab_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# weight from hidden to hidden for r gate\n",
    "Wr = tf.get_variable(\"Wr\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# bias for the r gate calculation\n",
    "br = tf.get_variable(\"br\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# weight from input to hidden\n",
    "Uh = tf.get_variable(\"Uh\", [vocab_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# recurrent weight matrix, hidden 2 hidden\n",
    "Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# bias for hidden matrix\n",
    "bh = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# output weight matrix\n",
    "Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "# bias for output matrix\n",
    "by = tf.get_variable(\"by\", [vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# perform the z gate calculation\n",
    "zt = tf.sigmoid(tf.matmul(xs_t, Wz) + tf.matmul(hs_t, Wz) + bz)\n",
    "# perform the r gate calculation\n",
    "rt = tf.sigmoid(tf.matmul(xs_t, Wr) + tf.matmul(hs_t, Wr) + br)\n",
    "# perform the hidden state calculation\n",
    "htilda_t = tf.tanh(tf.matmul(xs_t, Wh) + tf.matmul(tf.multiply(rt,hs_t), Wh) + bh)\n",
    "hs_t = tf.multiply((1 - zt), hs_t) + tf.multiply(zt, htilda_t)\n",
    "# perform the ouput calculation\n",
    "ys_t = tf.matmul(hs_t, Wy) + by\n",
    "# add the predicted character to the output list\n",
    "ys.append(ys_t)\n",
    "```\n",
    "\n",
    "A bit more involved now, but we've added the ability for our Recurrent Neural Network to handle longer term dependencies in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the place holders for our computational graph\n",
    "inputs = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='input')\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='targets')\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name='state')\n",
    "\n",
    "# create an initializer to init our weight matricies\n",
    "init = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "# set up our recurrent neural network and define the functions\n",
    "with tf.variable_scope(\"GRU\") as scope:\n",
    "    # hidden state at time t \n",
    "    hs_t = init_state\n",
    "    # list for output character predictions\n",
    "    ys = []\n",
    "    for t, xs_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0: scope.reuse_variables()\n",
    "            \n",
    "        # weight from input to hidden for z gate\n",
    "        Wxz = tf.get_variable(\"Wxz\", [vocab_size, hidden_size], initializer=init)\n",
    "        # weight from hidden to hidden for z gate\n",
    "        Whz = tf.get_variable(\"Whz\", [hidden_size, hidden_size], initializer=init)\n",
    "        # bias for the z gate calculation\n",
    "        bz = tf.get_variable(\"bz\", [hidden_size], initializer=init)\n",
    "        \n",
    "        # weight from input to hidden for r gate\n",
    "        Wxr = tf.get_variable(\"Wxr\", [vocab_size, hidden_size], initializer=init)\n",
    "        # weight from hidden to hidden for r gate\n",
    "        Whr = tf.get_variable(\"Whr\", [hidden_size, hidden_size], initializer=init)\n",
    "        # bias for the r gate calculation\n",
    "        br = tf.get_variable(\"br\", [hidden_size], initializer=init)\n",
    "        \n",
    "        # weight from input to hidden\n",
    "        Wxh = tf.get_variable(\"Wxh\", [vocab_size, hidden_size], initializer=init)\n",
    "        \n",
    "        # recurrent weight matrix, hidden 2 hidden\n",
    "        Whh = tf.get_variable(\"Whh\", [hidden_size, hidden_size], initializer=init)\n",
    "        # bias for hidden matrix\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=init)\n",
    "        \n",
    "        # output weight matrix\n",
    "        Why = tf.get_variable(\"Why\", [hidden_size, vocab_size], initializer=init)\n",
    "        # bias for output matrix\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer=init)\n",
    "        \n",
    "        # perform the z gate calculation\n",
    "        zt = tf.sigmoid(tf.matmul(xs_t, Wxz) + tf.matmul(hs_t, Whz) + bz)\n",
    "        # perform the r gate calculation\n",
    "        rt = tf.sigmoid(tf.matmul(xs_t, Wxr) + tf.matmul(hs_t, Whr) + br)\n",
    "        # perform the hidden state calculation\n",
    "        htilda_t = tf.tanh(tf.matmul(xs_t, Wxh) + tf.matmul(tf.multiply(rt,hs_t), Whh) + bh)\n",
    "        hs_t = tf.multiply((1 - zt), hs_t) + tf.multiply(zt, htilda_t)\n",
    "        # perform the ouput calculation\n",
    "        ys_t = tf.matmul(hs_t, Why) + by\n",
    "        # add the predicted character to the output list\n",
    "        ys.append(ys_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to keep track of our hidden states\n",
    "hprev = hs_t\n",
    "# apply the softmax output to the last output of our list\n",
    "output_softmax = tf.nn.softmax(ys[-1])\n",
    "\n",
    "# get all of the output characters together\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
    "\n",
    "# optimization algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "\n",
    "# clip the gradients\n",
    "grad_clipping = tf.constant(5.0, name='grad_clipping')\n",
    "clipped_grads = []\n",
    "for grad, var in grads:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads.append((clipped_grad, var))\n",
    "    \n",
    "# update the weights with gradient descent\n",
    "updates = optimizer.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, p: 0, loss: 4.168926\n",
      "----\n",
      " ;PPQYjoL!qnqglfrZJ,P:J;uf$,&KhRN!NEiXWn?JSXdiQgvACIZbTo;qu.slB:XOmlFaqBSCkjJLbi.?HUPy\n",
      "DfYhckXqLth&p,Vyx,LgrpbkEDzPdeoQitqEkDTRf::Q;AfSdbwCV3nKU$nIFshMatb?BJ;zCVTsZ'yhyZ?R\n",
      "q,;l !w;xcM&FI&TbV\n",
      "bZDkd;uX,X \n",
      "----\n",
      "\n",
      "iter: 500, p: 12500, loss: 3.298229\n",
      "----\n",
      " y t parr ahtstsdow:suass ahaaaew ceecaaezonunae edo\n",
      "\n",
      "BmMec'l, t sg tsmbnbnURal -h\n",
      "reso tneT  bft iy:no sd'?k npIceaem kSeSre voprs TfflcWr horrc lo aovns yisuddto ecrssiaue\n",
      " c!:Thehnn ctiu sn\n",
      "Toar st  \n",
      "----\n",
      "\n",
      "iter: 1000, p: 25000, loss: 2.441613\n",
      "----\n",
      " s, ohues them fetlom tlinis.\n",
      ": tislu otif rin tthieW bea, sh pind thar nou;gu, wndiNufTiiys:\n",
      "k\n",
      "!fg:taoreaeafutsou sir uh rewt wlat oisr jral y\n",
      "shesf\n",
      "mCrfu. \n",
      "olr tp,rmei.\n",
      ",s\n",
      "nol! tarrb,peis ow yok.c?n\n",
      " \n",
      "----\n",
      "\n",
      "iter: 1500, p: 37500, loss: 2.746028\n",
      "----\n",
      " r, Borlgwa\n",
      "s mamyog gint thr ftb kiss an tury wn,Bfv,\n",
      ", fodgt ar ii slk wattusl falHous finos won yasiak pule,\n",
      "Ths:\n",
      "SfctNet cenare\n",
      "s com 'it the. line snu, tefn m thcn uuh tell voulw unNins \n",
      "xllw dein \n",
      "----\n",
      "\n",
      "iter: 2000, p: 50000, loss: 2.416351\n",
      "----\n",
      " i&ling, cr AsmiriI St feas waet a:\n",
      "I Aives ,.\n",
      "\n",
      "EURCIAUS\n",
      ".Anl of almithin Mst heolg'lbt pe the theithere wit gougranot Ir\n",
      "heond leawcousel\n",
      "\n",
      "A'!.!UIE\n",
      "TlThe torsle thime\n",
      "hem an ceasdWcey\n",
      "Tos le sheliu or \n",
      "----\n",
      "\n",
      "iter: 2500, p: 62500, loss: 1.853010\n",
      "----\n",
      "  hut,\n",
      "Mzsrsto l pos ;oo?\n",
      "I I'. 's slaneCfed Iriand,\n",
      "Inf tiv sopecestve hy blmsuek\n",
      "As,\n",
      "\n",
      "Ay oowe: ho tle po,.\n",
      "\n",
      "IOuinF no wine? ha hon shod you stoll bltof hises de bit, oo !aens fou.\n",
      "IONgogl Iin tod nds \n",
      "----\n",
      "\n",
      "iter: 3000, p: 75000, loss: 1.816224\n",
      "----\n",
      " haso nhullll You feve shitheW veok s bperes, Semptheipet fivepciny tepy afnse chef.\n",
      "That, I ceu wheit pout that re 'utine-..\n",
      "Naco ponos gfcesg to mop kud thett tove bat lo\n",
      "emde\n",
      "Gcy iule\n",
      "Chan wamire, T \n",
      "----\n",
      "\n",
      "iter: 3500, p: 87500, loss: 1.942097\n",
      "----\n",
      " siy houtswint mace yut woy ony\n",
      "\n",
      "Cononr, h, wogr therr br wreatges\n",
      "\n",
      "VORTUSUS:\n",
      "Iy shadngs,rond shat sordand, I andur, poprleande,\n",
      "Arthans,\n",
      "\n",
      "habeille peat,\n",
      "I the, noured:\n",
      "And kallh for thas ar MarHe Dhe  \n",
      "----\n",
      "\n",
      "iter: 4000, p: 100000, loss: 2.105284\n",
      "----\n",
      " t hear\n",
      "Th? 'rat e lath\n",
      "Aelis the forst op ban, thech\n",
      "\n",
      "CRRIOIUS:\n",
      "Thout.\n",
      "\n",
      "SENIUS:\n",
      "Not cuthe peone had womed,\n",
      "And Ikf! tone or antthee \n",
      "oun\n",
      "Thin!\n",
      "Af beithy, thy whangifs momr you thon dowon hat the thav. \n",
      "----\n",
      "\n",
      "iter: 4500, p: 112500, loss: 1.816483\n",
      "----\n",
      " houerain, I werchen; hertisss\n",
      "Be; mar hith, arg alt ford houtertot at\n",
      "arn thoun f bat Minst meverouse,\n",
      "Cy I kitchiths and fond int oud ar fbetan\n",
      "Wheepir to micy a his make\n",
      ".\n",
      "Ohenvan:\n",
      "Ners.-\n",
      "ICasd zSt' \n",
      "----\n",
      "\n",
      "iter: 5000, p: 125000, loss: 2.060061\n",
      "----\n",
      "  doolle.\n",
      "Theiss de wo thate as ar:\n",
      "The poos o' dandeses worl, leore boks;\n",
      "And he thane he colles a has as\n",
      "Thoundn, ce bperger fore.\n",
      "\n",
      "CIIINIUS:\n",
      "Meeneans ardeFive hanct onl me,\n",
      "Whame!\n",
      "\n",
      "MININIUS:\n",
      "The rey \n",
      "----\n",
      "\n",
      "iter: 5500, p: 137500, loss: 2.360438\n",
      "----\n",
      " fu\n",
      "-Fave the lifthe ageit sus?\n",
      "\n",
      "oECINIAS:\n",
      "I matins: fer vishte ay ast\n",
      "Whous ann matisa f are ton:\n",
      "I Rs dlepeange's' g atest,\n",
      "And our\n",
      "tuws dod thist ve Roweifre'd\n",
      "Yet olle! wo han thius I are?\n",
      "\n",
      "SMMENIU \n",
      "----\n",
      "\n",
      "iter: 6000, p: 150000, loss: 2.344325\n",
      "----\n",
      " houre;:\n",
      "The luds she parowe\n",
      "Be nadent aeank.\n",
      "\n",
      "Fich muty felcoudirgery, laus,\n",
      "To ge hiruig sarsesd; linc mangfy.\n",
      "\n",
      "AUCINIUS:\n",
      "Whes iily haver-wyop ail hecril!\n",
      "\n",
      "BEsINIUS:\n",
      "Wherfhe ake be band hace,\n",
      "As, ant \n",
      "----\n",
      "\n",
      "iter: 6500, p: 162500, loss: 1.961284\n",
      "----\n",
      " he:\n",
      "No f oll althink pire, bans atere hfwt fenemst ford\n",
      "SiRod hexpialt?\n",
      "Wh\n",
      "And cur noqe nuund,, aft Lott aple\n",
      "Whik lisl. buose doopron dillln thusto!\n",
      "Te thatl seagli, the hot thae fal had ings.\n",
      "A dorg \n",
      "----\n",
      "\n",
      "iter: 7000, p: 175000, loss: 2.091956\n",
      "----\n",
      " ders, hat,\n",
      "Nel ap ou badd sous ateren;\n",
      "Shash my sall shame wis toungser\n",
      "And nobdest bhas, I'd be oln afr'y\n",
      "I hextcer eerdon morthen\n",
      "Forn mespnothas, and ast yours you reig\n",
      "Touch apth se so tiant and\n",
      "C \n",
      "----\n",
      "\n",
      "iter: 7500, p: 187500, loss: 2.412851\n",
      "----\n",
      " \n",
      "Lant your name munt marcis:\n",
      "Mur mither least's dis saw\n",
      "\n",
      "QAOE TE'RS:\n",
      "Loum, and hwirr ses urher;\n",
      "Vnkn, I acisell. I guistane, ats sy to'h!\n",
      "Anchor shathin cond my Mloge.\n",
      "\n",
      "QUkEN MERGAAEE:\n",
      "Wher on rugithi \n",
      "----\n",
      "\n",
      "iter: 8000, p: 200000, loss: 1.881766\n",
      "----\n",
      " EN\n",
      "BBR:\n",
      "Marde charitke thy feramou.\n",
      "\n",
      "Sicund ERnderey\n",
      "Do O ard endinders geadencands.\n",
      "\n",
      "ARAREMNE:\n",
      "Bmechses deams's nom sgall\n",
      "\n",
      "aAd oand hy comnbe?\n",
      "\n",
      "GLARMGAE:\n",
      "WiakHe of horco sow argire.\n",
      "I vi dome me armu \n",
      "----\n",
      "\n",
      "iter: 8500, p: 212500, loss: 2.102552\n",
      "----\n",
      " y ly-rees miclW your?\n",
      "Bnt wan wishaf liasegmage,\n",
      "Hather chas it wo gorvars me atith\n",
      "OD owhy padle peatfion had\n",
      "Hagr dlase freed? whik,\n",
      "Kpol nabingat sfake, fall bid.\n",
      "\n",
      "QUS:\n",
      "Shar wale not the dead, hot  \n",
      "----\n",
      "\n",
      "iter: 9000, p: 225000, loss: 1.963435\n",
      "----\n",
      " th\n",
      "And hy brindsents, Filshalad.\n",
      "Ss kelzect, and the thest?\n",
      "\n",
      "Tho laikiye! Gomeed my batines arf mine fomsgach.\n",
      "\n",
      "PCOlCESSI\n",
      "WhaR:\n",
      "Cod Gpell hell and?\n",
      "\n",
      "CIAtI HakDy hovepell,\n",
      "And sall ance ar yoy atief\n",
      "gr \n",
      "----\n",
      "\n",
      "iter: 9500, p: 237500, loss: 1.966576\n",
      "----\n",
      " tst the lorver\n",
      "Whef love at thy your jack?\n",
      "We's as at the lind.'\n",
      "Ih the tors your. UYese one, ere the hut,\n",
      "Thou your I popping to, ho dord of lims.\n",
      "\n",
      "HARCIDSS:\n",
      "Wow your die hait, to hash Syos, I stord. \n",
      "----\n",
      "\n",
      "iter: 10000, p: 250000, loss: 1.932790\n",
      "----\n",
      " iby.\n",
      "\n",
      "YLO CINGF:\n",
      "Then of laydst to to beak\n",
      "Bus my grvisndp; hitds the Mandeos intt\n",
      "\n",
      "LRSTERH:\n",
      "Bis jorge t;\n",
      "But'd theid this byestcundy\n",
      "Whinour bugllen,\n",
      "Weyr basinger Arkings fol\n",
      "Hes ane wat,-lovel of m \n",
      "----\n",
      "\n",
      "iter: 10500, p: 262500, loss: 2.356241\n",
      "----\n",
      " plict his.\n",
      "\n",
      "GAACHWHSY:\n",
      "What he bestangres you 'fe\n",
      "\n",
      "QhArd your, er-tureastchy\n",
      "Mring and Cpaall thenal.\n",
      "I Pact wiuk toRe you prote?\n",
      "\n",
      "CUSeETBOETE:\n",
      "And withtrirssew thak hoth\n",
      "\n",
      "Fors bethy iglming, and epto \n",
      "----\n",
      "\n",
      "iter: 11000, p: 275000, loss: 2.331776\n",
      "----\n",
      " :\n",
      "MTerserteree, my lood, firs,\n",
      "Thack sul semes'd lyad end\n",
      "Tham brecdray mace aot geomA\n",
      "\n",
      "HerizNn,\n",
      "Ay forme-trund mather tlroe dee.\n",
      "Ker; forqulomy I her mngitl.\n",
      "\n",
      "'REKSNI:\n",
      "Mely dreast jeart, m dith muind \n",
      "----\n",
      "\n",
      "iter: 11500, p: 287500, loss: 2.108641\n",
      "----\n",
      " me!\n",
      "\n",
      "QUEEN I I RercBHKI I chervevar,\n",
      "And be in'sile quave's shoth.\n",
      "\n",
      "KINGHAB HARWLINK:\n",
      "Thy mather doumh then I wat iton.\n",
      "\n",
      "QUEEN ELIZADI Hey Eens thetray tare.\n",
      "IKtoust wink bimy?\n",
      "\n",
      "KIOH RKIHHARB:\n",
      "Mavemy  \n",
      "----\n",
      "\n",
      "iter: 12000, p: 300000, loss: 2.731718\n",
      "----\n",
      " ish m'ss drae il to ent:\n",
      "Nepro ang be senfer ay my momosthit\n",
      "Wancas, livt an I grotge?\n",
      "\n",
      "Ford EivircEd: Groy thep my tee.\n",
      "\n",
      "sAEN: Rishindur,,\n",
      "Than ffar upperses?\n",
      "Ey ham Nhe bery weer Golm.\n",
      "Nuver he wom  \n",
      "----\n",
      "\n",
      "iter: 12500, p: 312500, loss: 2.251081\n",
      "----\n",
      " an.\n",
      "Ubells hom, ryces:\n",
      "Rotignde to Land forfer we.\n",
      "Whis herfer once us the cand.\n",
      "\n",
      "KING RACHARD III:\n",
      "Sin whew\n",
      "If Plofe, yourcy thy thall,\n",
      "Hove thow at I at hiods saend.\n",
      "\n",
      "CORCDS:\n",
      "Hom, I wishy rouver the \n",
      "----\n",
      "\n",
      "iter: 13000, p: 325000, loss: 2.304722\n",
      "----\n",
      " erfow.\n",
      "Thine fomsesusgan gheof lo fhalp.\n",
      "\n",
      "YORD LYARD II GoAdN HarciAkd aperse\n",
      "CagHastor ey furksough:\n",
      "Rich, ancealcider my to Whim,\n",
      "Yeer curyne ry lfick?\n",
      "My dove, lurdflbeike the gomce;\n",
      "Kere dela noor \n",
      "----\n",
      "\n",
      "iter: 13500, p: 337500, loss: 2.172187\n",
      "----\n",
      " be the fortrien.\n",
      "Neaks fall and enach herul\n",
      "Buth the enow as detoen, yog Soothe\n",
      "Oll Cacour the for with; your kand ast\n",
      "Weith ntouth whem to me rece\n",
      "Weiter, peistnd, a will, couderbunt sicald\n",
      "Thenca th \n",
      "----\n",
      "\n",
      "iter: 14000, p: 350000, loss: 2.014033\n",
      "----\n",
      " ut\n",
      "Herann coungpI nakns nody\n",
      " he twerw and supsead that Yours.\n",
      "We you fsatied, whencind which be I\n",
      "Raving that igrimy forst Nof.\n",
      "Os ole dane, whume shirk,\n",
      "Me ipge st fasthat, butp of uspe bead;\n",
      "And to \n",
      "----\n",
      "\n",
      "iter: 14500, p: 362500, loss: 1.417502\n",
      "----\n",
      " ince hath will\n",
      "\n",
      "HERRUFI:\n",
      "Hex whos nither whit shance;\n",
      "And yesangititht for sheic,\n",
      "Lors sraves. God vadhink prowi: do;BUver's lovitef hibpto-\n",
      "Dy with Goth our nobme, with thay ford, by whut ours:\n",
      "Ol th \n",
      "----\n",
      "\n",
      "iter: 15000, p: 375000, loss: 2.028230\n",
      "----\n",
      " o kill ouw it mus\n",
      "\n",
      "Torn Henjyee now me pingimis\n",
      "To my not nath foo the dreit;\n",
      "To leed inle that I hur be\n",
      "Butint orr lond, stold my leake fove al\n",
      "Bound crit it Eatles hearts\n",
      "Dote to me befs couts, wy c \n",
      "----\n",
      "\n",
      "iter: 15500, p: 387500, loss: 1.939101\n",
      "----\n",
      "  dos\n",
      "Tugis you hees dank rughs!\n",
      "Fof slend king ency opeal?\n",
      "Frosthes yorl wish, kich of me.\n",
      "Why canence sine the aund\n",
      "Of the terwess and and uppe.\n",
      "\n",
      "HETRUCBSIA:\n",
      "Mur aly loss in grayes ant\n",
      "Of I mell his, \n",
      "----\n",
      "\n",
      "iter: 16000, p: 400000, loss: 2.143670\n",
      "----\n",
      "  oprenof lowe save,\n",
      "Sir, with, my woord for beacome,\n",
      "Shase couliok'dly.\n",
      "\n",
      "ReEERE A:\n",
      "I reavling is freme se me\n",
      "Wht mystrde degelend's ig.\n",
      "\n",
      "Hand in mungers, and, hil it with smils.\n",
      "\n",
      "HINGBRUMELRY:\n",
      "Th ame  \n",
      "----\n",
      "\n",
      "iter: 16500, p: 412500, loss: 1.221715\n",
      "----\n",
      "  Jongbaits I words.\n",
      "When: foll amay woth his denday.\n",
      "Hacle, aw it tayay, seart:\n",
      "Ap kich ow terven: frie ere\n",
      "Troulds ham he thy beent tior's,\n",
      "Then, loak of be wnot berire\n",
      "Waly il ang and thoull be pand \n",
      "----\n",
      "\n",
      "iter: 17000, p: 425000, loss: 0.767454\n",
      "----\n",
      " k the byowst my jeace.\n",
      "\n",
      "KING RACHARD II:\n",
      "Tham is the somo' for,, fy lowst mulnt,\n",
      "Afas sthe wo tousat in anmes.\n",
      "\n",
      "KENG RICLARD IO:\n",
      "Rope pleeted the lord boke\n",
      "And nebring wee so joll:\n",
      "Dhand my blord, lor \n",
      "----\n",
      "\n",
      "iter: 17500, p: 437500, loss: 1.958604\n",
      "----\n",
      " e jant.\n",
      "\n",
      "HENRC BOLANGBRO:\n",
      "Heath hiss lat un tiln, grief thrued,\n",
      "Than in theme be thous, and sake on magn.\n",
      "\n",
      "DUCHESS\n",
      "ANGeRI ILs fate sward.\n",
      "\n",
      "DUMESLEO:\n",
      "All bothe?\n",
      "Tereis tan o frow, this have soamul\n",
      "Ol h \n",
      "----\n",
      "\n",
      "iter: 18000, p: 450000, loss: 2.053103\n",
      "----\n",
      " ar.\n",
      "\n",
      "QAENT:\n",
      ": ef,\n",
      "For yours Lostenash an slue!\n",
      "\n",
      "BQEKNS:\n",
      "Why racy, men:\n",
      "Roway hood rest wood velpe;\n",
      "Titrs monce tith their whis\n",
      "Yees\n",
      "Have the stiknder you? turs not. Brispo'd and at of awver,\n",
      "And fur f \n",
      "----\n",
      "\n",
      "iter: 18500, p: 462500, loss: 2.149080\n",
      "----\n",
      " of her\n",
      "Yoy, mughty.\n",
      "; whon the a army tearse uxte:\n",
      "Laan's you now hen nting nat CHors;\n",
      "An Jud it thene haw, grin.\n",
      "\n",
      "EADO CET LerY;\n",
      "No tay seagain, my pentembye take\n",
      "Hers, ince thou diven as scole.\n",
      "Shou \n",
      "----\n",
      "\n",
      "iter: 19000, p: 475000, loss: 2.155978\n",
      "----\n",
      "  to proonef of and\n",
      "We thay is out a hows\n",
      "\n",
      "Sended lord ats he fomour forle.\n",
      "Yom, my tooggrthat he fant.\n",
      "The woths ihen, beance be thote?\n",
      "\n",
      "ROMEO:\n",
      "O reco so him, hrove 'rish hup best\n",
      "And, to weer to wher \n",
      "----\n",
      "\n",
      "iter: 19500, p: 487500, loss: 1.466306\n",
      "----\n",
      "  in that cannist'\n",
      "That is upor honce lorgad anl\n",
      "Io ancainst, me knor as doner;\n",
      "Here freis aspere his arvies.\n",
      "\n",
      "RUMEO:\n",
      "O on rongige,\n",
      "Coware till to have ea; almVersof ablove\n",
      "This I leves corn hatk whowe \n",
      "----\n",
      "\n",
      "iter: 20000, p: 500000, loss: 1.634792\n",
      "----\n",
      " y fearll;\n",
      "As Rice.\n",
      "\n",
      "ROMEL:\n",
      "I soustich.\n",
      "\n",
      "MERCUTIO:\n",
      "Pary!'\n",
      "Yourn whond thun lath dale,\n",
      "Siment ye sone, ofn may.\n",
      "\n",
      "BEdIOL Shometes; my seak; I aw Whits of, thy lion.\n",
      "\n",
      "RUMEO:\n",
      "O han I\n",
      "on hand they yot For s \n",
      "----\n",
      "\n",
      "iter: 20500, p: 512500, loss: 1.482027\n",
      "----\n",
      " e\n",
      "on by that\n",
      "Thl sundy hou tree sid do sen.\n",
      "The sad me, out sinn, tence oud que is to mexdlagt.\n",
      "O; I say dage bite man the sot\n",
      "I with the filr you pat to-\n",
      "Jull? swell a by a let net gisel\n",
      "And never co \n",
      "----\n",
      "\n",
      "iter: 21000, p: 525000, loss: 1.673738\n",
      "----\n",
      " y\n",
      "Houll: appanistes's bant and,\n",
      "Hy wortstming I baot unay'Bud.\n",
      "This this obrouby or camp\n",
      "The more cen sie, tham bly thiselbact.\n",
      "Shel that tholl that, and beartonsute.\n",
      "\n",
      "RIMEO:\n",
      "O rry'd? 'tays alsedsid,  \n",
      "----\n",
      "\n",
      "iter: 21500, p: 537500, loss: 1.400743\n",
      "----\n",
      " hem.\n",
      "\n",
      "JULIET:\n",
      "Fert, ir may.\n",
      "\n",
      "JULIE::\n",
      "Now, hand heer jurtlemy tlue.\n",
      "\n",
      "JALAUT:\n",
      "Athour,\n",
      "Uugher challemang then, may.\n",
      "\n",
      "LOUYE:\n",
      "O, wull you modn with my tobe,\n",
      "God be natule bid thou you.\n",
      "How, Lyppunitrience, \n",
      "----\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5be522cb56fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                                       feed_dict={inputs: input_vals,\n\u001b[1;32m     31\u001b[0m                                                  \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                                  init_state: hprev_val})\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/florian/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now that all the functions are set up we can run this thing\n",
    "\n",
    "# function to one hot encode the characters\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Initial values\n",
    "n, p = 0, 0\n",
    "hprev_val = np.zeros([1, hidden_size])\n",
    "\n",
    "while True:\n",
    "    # Initialize\n",
    "    if p + seq_length + 1 >= len(shakes) or n == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        p = 0  # reset\n",
    "\n",
    "    # Prepare inputs\n",
    "    input_vals = [char2idx[ch] for ch in shakes[p:p + seq_length]]\n",
    "    target_vals = [char2idx[ch] for ch in shakes[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    input_vals = one_hot(input_vals)\n",
    "    target_vals = one_hot(target_vals)\n",
    "\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updates],\n",
    "                                      feed_dict={inputs: input_vals,\n",
    "                                                 targets: target_vals,\n",
    "                                                 init_state: hprev_val})\n",
    "    if n % 500 == 0:\n",
    "        # Progress\n",
    "        print('iter: %d, p: %d, loss: %f' % (n, p, loss_val))\n",
    "\n",
    "        # Do sampling\n",
    "        sample_length = 200\n",
    "        start_ix = random.randint(0, len(shakes) - seq_length)\n",
    "        sample_seq_ix = [char2idx[ch] for ch in shakes[start_ix:start_ix + seq_length]]\n",
    "        idxs = []\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "\n",
    "        for t in range(sample_length):\n",
    "            sample_input_vals = one_hot(sample_seq_ix)\n",
    "            sample_output_softmax_val, sample_prev_state_val = \\\n",
    "                sess.run([output_softmax, hprev],\n",
    "                         feed_dict={inputs: sample_input_vals, init_state: sample_prev_state_val})\n",
    "\n",
    "            ix = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "            idxs.append(ix)\n",
    "            sample_seq_ix = sample_seq_ix[1:] + [ix]\n",
    "\n",
    "        txt = ''.join(idx2char[ix] for ix in idxs)\n",
    "        print('----\\n %s \\n----\\n' % (txt,))\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
